{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import time\n",
    "import os\n",
    "import ast\n",
    "\n",
    "from api_handling import api_handling\n",
    "# from Song_Consolidation import Song_Consolidation\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = api_handling(save_location='../Data/')\n",
    "data.csv_pull(overwrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This below creates the time series index, to use to sample the other data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_TS_map(duration_ts, sample_rate):\n",
    "    #sample_rate is in samples per second, which get converted to ms in the method\n",
    "    sample_rate = sample_rate / 1000\n",
    "    # ms\n",
    "    step = 1/sample_rate\n",
    "    time_row = list(np.arange(start=step, stop = duration_ts, step = step))\n",
    "    return time_row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See Below: Given a traverses the passed feature and it scales the time series data and turns it into an array of time series samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_TS_map(column_value_map, time_series_map):\n",
    "    '''\n",
    "    To be mapped on every individual song's column on a dataframe\n",
    "        - such that the input value is a list of dictionaries \n",
    "    '''\n",
    "    start_output = list(np.zeros(len(time_series_map)))\n",
    "    duration_output = list(np.zeros(len(time_series_map)))\n",
    "\n",
    "    if type(column_value_map) == str:\n",
    "        column_value_map = ast.literal_eval(column_value_map)\n",
    "    assert type(column_value_map) == list, 'reassess the data type being time-mapped'\n",
    "    assert type(column_value_map[0]) == dict, 'events are not stored as dictionaries'\n",
    "\n",
    "    time_axis_last_applied = 0\n",
    "    # helps with time complexity due to events occuring in sequential order as delivered by the spotify api\n",
    "    # when searching in the time series, will only search forward from the last point in time when we last applied an item\n",
    "\n",
    "    step_size = time_series_map[0]\n",
    "    # if sample time-signature minus event start is less than or equal to the step size, it means that the event occured during that time interval \n",
    "\n",
    "    for event in column_value_map:\n",
    "        # every tatum in the list, represented as a dictionary, for example\n",
    "        \n",
    "        #converts into milliseconds\n",
    "        event_start = event['start'] * 1000\n",
    "        event_duration = event['duration'] * 1000\n",
    "        \n",
    "\n",
    "        for t in range(time_axis_last_applied, len(time_series_map)):\n",
    "            #iterate through every reasonable time sample available in the song as created by the input time series map, and check if an event happened in the interval\n",
    "            \n",
    "            sample_time = time_series_map[t]\n",
    "            time_axis_last_applied = t\n",
    "            # api provides data squentially so can update, therefore we won't check the same time twice by reassigning every iteration\n",
    "\n",
    "            if np.abs(sample_time - event_start) <= step_size and sample_time >= event_start:\n",
    "                \n",
    "                # applying the start event explicitly\n",
    "                start_output[t] = 1 \n",
    "                #applying duration at following time events iteratively \n",
    "                remaining_duration = float(event_duration)\n",
    "                temp_index = int(t)\n",
    "                while remaining_duration > 0:\n",
    "                    # checking for index issues\n",
    "                    if temp_index >= len(time_series_map):\n",
    "                        break\n",
    "                    else:\n",
    "                        duration_output[temp_index] = 1\n",
    "                        remaining_duration -= step_size\n",
    "                        temp_index += 1\n",
    "\n",
    "                break\n",
    "\n",
    "    return [start_output, duration_output]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See Below: Given a 'segments' cell which has a dictionary in it, it scales the time series data and turns it into an array of time series samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_loudness_vector(column_value_map, time_series_map):\n",
    "    output = list(np.zeros(len(time_series_map)))\n",
    "    change_vector = list(np.zeros(len(time_series_map)))\n",
    "\n",
    "\n",
    "    if type(column_value_map) == str:\n",
    "        column_value_map = ast.literal_eval(column_value_map)\n",
    "    assert type(column_value_map) == list, 'reassess the data type being time-mapped'\n",
    "    assert type(column_value_map[0]) == dict, 'events are not stored as dictionaries'\n",
    "\n",
    "    time_axis_last_applied = 0\n",
    "    # helps with time complexity due to events occuring in sequential order as delivered by the spotify api\n",
    "    # when searching in the time series, will only search forward from the last point in time when we last applied an item\n",
    "\n",
    "    step_size = time_series_map[0]\n",
    "\n",
    "    begin_lagging = False\n",
    "    lagging_value = None\n",
    "    # continuous loudness value lagging begins once we find the first, \n",
    "    # then will continue imputing that value until next loundness value is found \n",
    "\n",
    "\n",
    "    for event in column_value_map:\n",
    "        assert type(event) == dict\n",
    "        # every song segment\n",
    "        \n",
    "        #converts into milliseconds\n",
    "        event_start = event['start'] * 1000.0\n",
    "        event_duration = event['duration'] * 1000.0\n",
    "        event_end = event_start + event_duration\n",
    "        event_max_time = event['loudness_max_time'] * 1000.0\n",
    "\n",
    "        start_loudness = float(event['loudness_start']) \n",
    "        assert type(start_loudness) == float\n",
    "        max_loudness = float(event['loudness_max']) \n",
    "        end_loudness = float(event['loudness_end'])\n",
    "\n",
    "        for (loudness_value, loudness_time) in zip(\n",
    "            [start_loudness, max_loudness, end_loudness],\n",
    "            [event_start, event_max_time, event_end]):\n",
    "            # I have loudness values for each of these elements\n",
    "\n",
    "            for t in range(time_axis_last_applied, len(time_series_map)):\n",
    "                # iterate through every reasonable time sample available in the song as created by the input time series map,\n",
    "                # and check if an event happened in the interval\n",
    "                sample_time = time_series_map[t]\n",
    "                time_axis_last_applied = t\n",
    "\n",
    "                if np.abs(sample_time - loudness_time) <= step_size and sample_time >= loudness_time:\n",
    "                    # applying the start event explicitly\n",
    "                    output[t] = float(loudness_value)\n",
    "                    begin_lagging = True\n",
    "                    lagging_value = float(loudness_value)\n",
    "                    break\n",
    "                else:\n",
    "                    if begin_lagging:\n",
    "                        output[t] = float(lagging_value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See Below: Helper Methods to traverse the rows, or songs, in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_song_ts_data_with_duration(row, columns, sample_rate):\n",
    "    '''for a given row, and sent columns, returns the song ts data formatted for GAN training'''\n",
    "    song_time_map = create_TS_map(row['duration_ms_x'], sample_rate=sample_rate)\n",
    "    output = []\n",
    "    for column in columns:\n",
    "        for o in column_TS_map(row[column],time_series_map = song_time_map):\n",
    "            output.append(o)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_song_ts_data_no_duration(row, columns, sample_rate):\n",
    "    '''for a given row, and sent columns, returns the song ts data formatted for GAN training'''\n",
    "    song_time_map = create_TS_map(row['duration_ms_x'], sample_rate=sample_rate)\n",
    "    output = []\n",
    "    for column in columns:\n",
    "        if column == 'segments':\n",
    "            output.append(construct_loudness_vector(row[column],time_series_map=song_time_map))\n",
    "        else:\n",
    "            output.append(column_TS_map(row[column],time_series_map = song_time_map)[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_song_single_data(row,columns):\n",
    "    output = []\n",
    "    for column in columns:\n",
    "        output.append(row[column])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(df, single_unit_columns, time_series_columns_with_duration, time_series_columns_no_duration, ts_sample_rate, split, verbose = False):\n",
    "    '''\n",
    "    Pulls all data as categorized,\n",
    "    reason for no_duration is that section data is that ts section data is less valuable, as the beginning of a new section implies that the duration has concluded so one list is valuable at most\n",
    "    '''\n",
    "    for col in single_unit_columns + time_series_columns_no_duration + time_series_columns_with_duration:\n",
    "        assert col in df.columns, 'send valid columns'  \n",
    "    \n",
    "    if split:\n",
    "        single_data = []\n",
    "        ts_data = []\n",
    "        for row_index in range(df.shape[0]):\n",
    "            print(f'\\r Progress: {row_index*100/df.shape[0]}%', end='')\n",
    "            row = df.iloc[row_index]\n",
    "            single_data.append(prepare_song_single_data(row,single_unit_columns))\n",
    "            ts_data.append(prepare_song_ts_data_with_duration(row=row,columns=time_series_columns_with_duration, sample_rate=ts_sample_rate) + prepare_song_ts_data_no_duration(row=row,columns=time_series_columns_no_duration, sample_rate=ts_sample_rate))\n",
    "        if verbose:\n",
    "            return single_data, ts_data, single_unit_columns + time_series_columns_with_duration + time_series_columns_no_duration\n",
    "        else: \n",
    "            return single_data, ts_data\n",
    "\n",
    "    else:\n",
    "        output = []\n",
    "        for row_index in range(df.shape[0]):\n",
    "            row = df.iloc[row_index]\n",
    "            output.append(prepare_song_single_data(row,single_unit_columns) + prepare_song_ts_data_with_duration(row=row,columns=time_series_columns_with_duration, sample_rate=ts_sample_rate) + prepare_song_ts_data_no_duration(row=row,columns=time_series_columns_no_duration, sample_rate=ts_sample_rate))\n",
    "        if verbose:\n",
    "            return output, single_unit_columns+time_series_columns_with_duration+time_series_columns_no_duration\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Progress: 99.98551774076756%%%"
     ]
    }
   ],
   "source": [
    "training_data = load_training_data(\n",
    "    data.frames['full_song_data'],\n",
    "    single_unit_columns=['popularity','tempo','valence'],\n",
    "    time_series_columns_with_duration=['beats','tatums','bars'],\n",
    "    time_series_columns_no_duration=['segments','sections'],\n",
    "    ts_sample_rate=10,\n",
    "    split=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_2.pickle', 'wb') as f:\n",
    "    pickle.dump(training_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "167cee73ec1a961f9ac1fcb8b2030bcc34926d11821ce746877c905156829734"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
